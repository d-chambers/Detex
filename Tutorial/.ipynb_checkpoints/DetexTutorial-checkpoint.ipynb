{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Detex Tutorial\n",
    "\n",
    "Detex is a python package for performing waveform similarity clustering and subspace detection. It is written in python and relies heavily on Obspy, Numpy, Scipy, Matplotlib, and Pandas. If you are not familiar with python I recommend you install the anaconda distribution for your platform (http://continuum.io/downloads) and spend a few hours learning the basics of the language before attempted to use Detex. Here are some great tutorials:\n",
    "\n",
    "http://www.stavros.io/tutorials/python/\n",
    "http://www.tutorialspoint.com/python/python_quick_guide.htm\n",
    "\n",
    "Also, any time spent learning obspy (http://docs.obspy.org/tutorial/) is a great investment as it is a very powerful tool for geophysical processing.\n",
    "\n",
    "Some knowledge of pandas will also be useful, as the pandas DataFrame is used extensively in detex (http://pandas.pydata.org/pandas-docs/stable/tutorials.html).\n",
    "\n",
    "Special thanks to Tex Kubacki (whose work inspired Detex), Jared Stein, Lisa Linvile, Shawn Blotz, and Chase Batchelor.\n",
    "\n",
    "## Installation \n",
    "\n",
    "Detex can be installed by running the setup.py script in the distribution directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "The basic Detex workflow has five steps:\n",
    "\n",
    "1. Prepare required files\n",
    "2. Data acquisition\n",
    "3. Clustering\n",
    "4. Subspace detection\n",
    "5. Detection association\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "##1. Prepare required files\n",
    "There are two required files: the station key and the template key.\n",
    "\n",
    "The station key is generally saved as StationKey.csv. The following is an example:\n",
    "\n",
    "\n",
    "| NEWTORK | STATION | STARTTIME | ENDTIME | LAT | LON | ELEVATION | CHANNELS |\n",
    "|:----------:|:-----------:|:------------:| :-: | :-: | :-: | :-: |\n",
    "| TA | M18A\t| 2009-04-01T00:00:00 | 2009-04-04T00:00:00\t| 41.4272 | -110.0674 | 2103 | BHE-BHN-BHZ |\n",
    "| TA | M17A\t| 2009-04-01T00:00:00 | 2009-04-04T00:00:00\t| 41.4729 | - 110.6664 | 2101 | BHE-BHN-BHZ |\n",
    "\n",
    "The STARTTIME and ENDTIME fields indicated the time range of the continuous data and can be in any format readable by the obspy.UTCDateTime class (including a time stamp). See the obspy.UTCDateTime docs for more info (http://docs.obspy.org/packages/autogen/obspy.core.utcdatetime.UTCDateTime.html)\n",
    "\n",
    "The CHANNELS field should list the channels that will be used for each station separated by a dash (-). Additionally, any extra fields can be added without affecting Detex's ability to read the file.\n",
    "\n",
    "The template key by default is saved as TemplateKey.csv. It contains information on each of the events that will be used to scan the continuous data for previously undetected events. Here is a few lines of the template key included in this tutorial:\n",
    "\n",
    "| CONTRIBUTOR | NAME | TIME | LAT | LON | DEPTH | MTYPE | MAG |\n",
    "| :--: | :--: | :--: | :--: | :--: |:--: | :--: |:--: |\n",
    "| ANF | 2007-12-19T17-56-18 | 2007-12-19T17-56-18 | 41.7205\t| -110.6486\t| 4.07 | ML | 2.36 |\n",
    "| ANF | 2007-12-21T18-30-09\t| 2007-12-21T18-30-09 | 41.7669\t| -110.6122\t| 8.97 | ML | 2.17 |\n",
    "| ANF | 2007-12-21T18-30-09\t| 2007-12-21T18-30-09 | 41.7669\t| -110.6122\t| 8.97 | ML\t| 2.17 |\n",
    "\n",
    "The CONTRIBUTOR, MTYPE, and DEPTH fields are not required but can be useful for record keeping. Additionally, any extra fields can be added in any order in order to better keep track of the events. \n",
    "\n",
    "The NAME field can be any string that can also be used as a file name by your OS. Windows does not allow \":\" in a file path so the \":\" between the hour and minute, and between the minute and seconds, have been replaced with a \"-\". Again, the time field can be in any format understood by obspy.UTCDateTime.\n",
    "\n",
    "The LAT and LON fields are not strictly required for basic Detex functionality, but are used in some visualization methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Data aquisition\n",
    "Detex uses obspy's fdsn module to download seismic data from a variety of data centers. For supported options see the obspy documentation (http://docs.obspy.org/packages/obspy.fdsn.html)\n",
    "\n",
    "The getdata module is used for acquisition. Once the template key and station key have been created the data can be downloaded by using the getAllData function. Progress will periodically print to screen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Getting template waveforms"
     ]
    }
   ],
   "source": [
    "import detex #import detex module\n",
    "# the next line is only needed in ipython notebook to make sure all figures show inline rather than opening seperate windows\n",
    "%pylab inline \n",
    "detex.getdata.getAllData() #download all data from iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  3. Clustering\n",
    "The next step is to cross correlate every event with every other event in order to form waveform similarity groupings on each station. A single link-algorithm is used to perform the clustering up to a determined dissimilarity level.   \n",
    "\n",
    "In order to do this a clusterStream object is created, which is essentially a container for cluster objects created using data from each station independently. The main input parameter is the required correlation coefficient, below which clustering will not occur. If you want to run each waveform as a 1D subspace (IE in waveform correlation detection) you can simply set the required correlation coefficient to 1. Conversely, if you want to include all events in the subspace regardless of similarity then set this parameter to 0. The default value is 0.5 can be easily changed without re-running the correlations. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl=detex.subspace.createCluster() # Create a clusters stream object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl.dendro() #create a dendrogram to visualize grouping structure on each station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we wanted to form strictly 4 groups on each station we can modify the required correlation coefficient for grouping. This can be done for all stations at once or for each station individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl.updateReqCC(.55)\n",
    "cl.dendro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl['TA.M17A'].updateReqCC(.38) # set required correlation coef. for only station TA.M17A\n",
    "cl['TA.M17A'].dendro() # visualize grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Similarity matricies can also be generated\n",
    "cl.simMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several other functions of the ClusterStream class. Notably, input for hypoDD (a well-established double difference relocation program) can be created using the writeHypoDDEventInput, writeHypoDDStationInput, and writeSimpleHypoDDInput class methods; although as of version 0.1.0 they have not been fully tested. I hope to develop other methods for locating detected events in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Subspace detection\n",
    "The subspace creation process is applied to each waveform similarity group. The process involves 1) aligning the waveforms to optimize similarity, 2) performing a singular value decomposition, 3) determining a required dimension of representation, and 4) setting a significant detection statistic threshold. As a final step 5) the subspace detectors are run on each station and saved to an SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, the creation of the SubSpaceStream\n",
    "ss= detex.subspace.createSubSpace() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Trim waveforms\n",
    "The next step is to set the beginning trim time for each subspace group and un-clustered singleton on each station. This can be done by calling GUI based pick functions built into the SubSpaceStream class (here it would be ss.detex.pickTimes()),or by attaching a csv or pickled pandas data frame with the following populated fields populated for at least one phase of each event-station pair:\n",
    "TimeStamp, Station, Event, Phase. \n",
    "\n",
    "The EventPicks.pkl file included in the tutorial is such a file (you can create this file by calling detex.util.pickPhases or load the dataframe by calling pandas.read_csv). \n",
    "\n",
    "Detex will then find the first arriving phase for each waveform (event-station pair) and average for the entire aligned group. From the average first arrival sample the waveforms will be trimmed to some duration ( 30 seconds default) or to the last arriving phase. See the function docs for further details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss.attachPickTimes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2, 4.3, 4.4 Perform SVD, set dimension of representation, and set threshold\n",
    "Next a singular value decomposition is performed on the  waveform groups that have been aligned and trimmed. A dimension of representation (IE the number of left singular vectors used to describe the waveform family) is calculated based on the fractional energy capture of 90% (by default). A detection statistic (DS) threshold for each subspace and singleton is then determined by calculated the detection statistic of the subspace with random continuous data that contains no high amplitude signals, fitting a beta PDF to the distribution, and finding the DS corresponding to the selected probability of false detection $10^{-12}$ by default). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss.SVD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###4.5 Run detectors\n",
    "Detex will scan the continuous data for each station-subspace pair and declare a detection whenever any subspace's threshold is exceeded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss.detex(useSingles=True) # run subspace detections and also run unclustered events as 1D subspaces (IE waveform correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are saved to an sqlite database. The following tables are saved in the database (named SubSpace.db by default):\n",
    "\n",
    "| Table | Description |\n",
    "|:-----:| :---------: |\n",
    "| ss_df | Results of the detections for the subspaces |\n",
    "| sg_df | Detection results for the singletons (un-clustered events) |\n",
    "| filt_params | Filter parameters used for the detections |\n",
    "| ss_info | General information about each of the subspaces (such as station, comprising events, thresholds, etc.) |\n",
    "| sg_info | General information about each singleton |\n",
    "| ss_hist | Binned counts of all detection statistic values for subspaces |\n",
    "| sg_hist | Binned counts of all detection statistic values for singletons |\n",
    "\n",
    "Any of these tables can be loaded into a dataframe using the detex.util.loadSQLite function. For example, if we wanted to make an ugly plot of all of the detection statistic values for the subspaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #import matplotlib for visualization\n",
    "import json # used to convert string arrays as loaded from sql to numpy arrays\n",
    "\n",
    "hist=detex.util.loadSQLite('SubSpace.db','ss_hist') # load the ss_hist table of the SubSpace.db sqlite database\n",
    "\n",
    "for ind,row in hist.iterrows(): #loop over datarame\n",
    "    hist.loc[ind,'Value']=np.array(json.loads(row.Value)) #convert string arrays into numpy arrays \n",
    "\n",
    "avbins=(hist.iloc[0].Value[:-1]+hist.iloc[0].Value[1:])/2.0 # middle of bin values for histograms\n",
    "\n",
    "\n",
    "## Plot each histogram\n",
    "for ind, row in hist.iterrows(): # loop again through dataframe\n",
    "    if ind==0: # skip if index of dataframe is 0 (these are the bin values)\n",
    "        continue \n",
    "    plt.plot(avbins,row.Value,label=row.Sta+':'+row.Name) #plot\n",
    "plt.xlabel('Detection Statistic') #label x\n",
    "plt.ylabel('Occurrence Rate') # label y\n",
    "plt.title('Detection Statistic') # lable title\n",
    "plt.legend(loc='center left',bbox_to_anchor=(1, 0.5)) #add legend outside of plot\n",
    "plt.semilogy() #use semilog on y axis\n",
    "plt.xlim([0,.5]) #set x lim\n",
    "\n",
    "plt.show() #show plot\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Associate detections\n",
    "\n",
    "The detex module \"results\" is used to associate all of the detections (DS that exceeded the determined threshold) on various stations together into coherent events. The association requirement is an overlap in predicted origin times. If a verification data set (IE event ground-truth) is available it can be used to assess detector performance. \n",
    "\n",
    "Note: If possible, it is very important to use at least 2 stations separated in space in order to reduce false detections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# associate events together, only require events to occur on 2 stations (that's all we have), count detections verified if they\n",
    "# occure within 10 minutes (5 minutes on either side) of reported origin times in blasting catalog. \n",
    "res=detex.results.detResults(requiredNumStations=2,veriBuffer=60*10,veriFile='veriFile.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The associations may take a while for large data sets, there are still some optimizations that need to be implemented.\n",
    "The verified detections, new detections, and auto detections (detection of training events) are stored in the form of pandas DataFrames that can be accessed by the Vers, Dets, and Autos attributes of the DetResults object. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res.Dets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res.Autos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res.Vers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Loading the mine's blast log we can see that all six blasts over the four days were successfully detected with no false detections. Note: the magnitude, latitude, longitude, and depth were not known so I simply use dummy values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "log=pd.read_csv('veriFile.csv')\n",
    "log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only required the detections to occur on one station, however, (even with the default ultra-conservative acceptable probability of false detection of $10^{-12}$ the detector will return many false detections. For this reason it is important to use more than one station when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import detex\n",
    "res=detex.results.detResults(requiredNumStations=1,veriBuffer=60*10,veriFile='veriFile.csv')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have detected new events we can instruct detex to extract the waveforms of the new detections. With the extracted waveforms phase picks can be made in order to located the newly-found events, cross correlation lag times can be calculated with the clustering and the detected events can be used to create a new detector to potentially find more events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res.writeDetections(eventDir='DetectedEvents',updateTemKey=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the waveforms of the newly detected events have been stored with the same directory structure as the TemplateWaveForms directory in a directory named DetectedEvents (because this is the argument we assigned to it). A new template key of the detected events can also be created, or by default the current template key csv will be updated with the newly detected events. The naming convention is the same but detected events will have a lowercase \"d,\" for detected, at the start of the name string. The entire process can then be repeated to try and detect even more events, but in this case we know we have found all that there is to find. \n",
    "\n",
    "Note that detex has created a log (detex_log.log) that can be useful in debugging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detex utilities\n",
    "There are several useful utilites in the detex.util module. Here I will highlight a few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KML generation\n",
    "Using the simple kml module (https://pypi.python.org/pypi/simplekml/1.2.8) several detex files can be converted to kml for easy viewing in google earth. They include the stations in the station key (detex.util.writeKMLFromStationKey), the template key (detex.util.writeKMLFromTemplateKey), outputs from hypoDD or hypoInverse, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading arbitrary continuous data\n",
    "The loadContinuousData function of detex.util can be used to load any data contained in the ContinuousWaveForms directory, trimmed to a user's specifications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
